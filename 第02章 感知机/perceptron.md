$$
ŷ = sign(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)
$$

sign 是符号函数，正数输出+1，负数输出-1

1. **输入特征（x₁,x₂...xₙ）**
   例如判断是否是优质苹果：
   - x₁=甜度（0-10分）
   - x₂=色泽鲜艳度（0-10分）
   - x₃=果径大小（cm）
2. **权重（w₁,w₂...wₙ）**
   每个特征的"重要性分数"：
   - w₁=2（甜度最重要）
   - w₂=1.5（色泽次之）
   - w₃=0.5（大小影响小）
3. **偏置项 b**
   相当于"基础分"，例如 b=-5（整体标准较高） 

## 损失函数

$$
L(\mathbf{w}, b) = \sum_{i=1}^{N} \max(0, -y_i (\mathbf{w} \cdot \mathbf{x}_i + b))
$$

- $y_i$是第 i个样本的真实标签。 $w*x_i+b$是感知机预测值

​       感知机判断和真实标签相反，$y_i (\mathbf{w} \cdot \mathbf{x}_i + b)$为负

通过不断调整 $w$和 $b$ 来极小化损失函数，使得误分类样本的数量逐渐减少

- 对于错误分类的点 ($x_i$,$y_i$)，其到分离超平面的距离为：

$$
\frac{|\mathbf{w} \cdot \mathbf{x}_i + b|}{\| \mathbf{w} \|}
$$

$$
\frac{-y_i*(\mathbf{w} \cdot \mathbf{x}_i + b)}{\| \mathbf{w} \|}
$$

  w:每个特征的权重组成的向量



## 随机梯度感知机学习算法

- 如果样本 (xi,yi)被正确分类（即$y_i(w*x_i+b)>=0)$梯度为 0，不更新参数。

- 如果样本 (xi,yi) 被错误分类（即 yi(w⋅xi+b)<0），梯度为：

- $$
  \nabla_{\mathbf{w}} L(\mathbf{w}, b) = -y_i \mathbf{x}_i
  $$

- $$
  \nabla_{b} L(\mathbf{w}, b) = -y_i
  $$

###### 感知机的参数更新规则    *η* 是学习率，控制参数更新的步长。每次迭代时，随机选择一个样本计算梯度，而不是使用全部样本。

$$
\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i
$$

$$
b \leftarrow b + \eta y_i
$$

##  感知机学习算法的收敛性

感知机学习算法通过不断调整参数 **w** 和 b来减少错误分类样本的数量。当数据集线性可分时，算法最终会找到一个能够完全正确分类所有样本的超平面。

错误分类次数 k是指感知机算法在训练过程中对样本进行错误分类的总次数。当数据集线性可分时，错误分类次数 k满足以下 上界：
$$
k \leq \left( \frac{\gamma}{R} \right)^2
$$

- R 是样本特征向量的最大范数，即：(反映了数据集的规模)
  $$
  R = \max_i \| x_i \|
  $$
  *γ* 是样本到理想分离超平面的最小间隔（几何间隔），即：(反映了数据集的分离难度。间隔越大，数据集越容易分离。)
  $$
  \gamma = \min_i \frac{y_i (w^* \cdot x_i + b^*)}{\| w^* \|}
  $$

  - 数据集规模越大（*R* 越大），错误分类次数可能越多。算法收敛越慢
  - 数据集越容易分离（γ* 越大），错误分类次数越少。算法收敛越快

